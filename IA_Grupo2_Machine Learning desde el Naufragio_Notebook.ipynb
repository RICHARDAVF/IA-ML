{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grupo 2**\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "1. Aviles Ferro, Richard\n",
    "\n",
    "2. Chuquimantari Villegas, Freddy\n",
    "\n",
    "3. Huallipe Menez, Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#graficas\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#modelo machine learning\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "#importamos librerias\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from subprocess import check_output\n",
    "#lectura de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import time\n",
    "#graficas\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#modelo machine learning\n",
    "from sklearn import model_selection,feature_selection,metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datos\n",
    "data_original=pd.read_csv(\"train.csv\")\n",
    "data_test=pd.read_csv(\"test.csv\")\n",
    "data_copy=data_original.copy(deep=True)\n",
    "data_combinada=[data_copy,data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_original.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diccionario de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Definición | \n",
    "| --- | --- |\n",
    "| PassengerId | Identificador |\n",
    "| Survived | Sobrevivientes |\n",
    "| Pclass | Categoría social |\n",
    "| Name | Nombre del pasajero |\n",
    "| Sex | Sexo del pasajero |\n",
    "| Age | Edad del pasajero |\n",
    "| SibSp | Número de hermanos/cónyugues a bordo |\n",
    "| Parch | Número de padres/hijos a bordo |\n",
    "| Ticket | Código de ticket |\n",
    "| Fare | Tarifa del ticket |\n",
    "| Cabin | Código de cabina |\n",
    "| Embarked | Puerto de embarque |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputacion de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_combinada:\n",
    "    #Completando datos con la mediana\n",
    "    data[\"Age\"].fillna(data[\"Age\"].median(),inplace=True)\n",
    "    #completando datos con la moda\n",
    "    data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0],inplace=True)\n",
    "    #completandon datos con la mediana\n",
    "    data[\"Fare\"].fillna(data[\"Fare\"].median(),inplace=True)\n",
    "#elimanamos datos que no son necesarios\n",
    "datos_eliminar=[\"PassengerId\",\"Cabin\",\"Ticket\"]\n",
    "data_copy.drop(datos_eliminar,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_combinada:\n",
    "    data[\"TamFamilia\"]=data[\"SibSp\"]+data[\"Parch\"]+1\n",
    "    data[\"IsAlone\"]=1\n",
    "    data[\"IsAlone\"].loc[data[\"TamFamilia\"]>1]=0\n",
    "    data[\"Titulo\"]=data[\"Name\"].str.split(\", \",expand=True)[1].str.split(\".\",expand=True)[0]\n",
    "    data[\"FareBin\"]=pd.qcut(data[\"Fare\"],4)\n",
    "    data[\"AgeBin\"]=pd.cut(data[\"Age\"].astype(int), 5)\n",
    "cantidad_minima=10\n",
    "name_titulo=(data_copy[\"Titulo\"].value_counts()<cantidad_minima)\n",
    "#print(name_titulo)\n",
    "data_copy[\"Titulo\"]=data_copy[\"Titulo\"].apply(lambda x: \"Misc\" if name_titulo.loc[x]==True else x)\n",
    "data_copy[\"Titulo\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "etiqueta=LabelEncoder()\n",
    "for data in data_combinada:\n",
    "    data[\"SexCode\"]=etiqueta.fit_transform(data[\"Sex\"])\n",
    "    data[\"EmbarkedCode\"]=etiqueta.fit_transform(data[\"Embarked\"])\n",
    "    data[\"TituloCode\"]=etiqueta.fit_transform(data[\"Titulo\"])\n",
    "    data[\"AgeBinCode\"]=etiqueta.fit_transform(data[\"AgeBin\"])\n",
    "    data[\"FareBinCode\"]=etiqueta.fit_transform(data[\"FareBin\"])\n",
    "objetivo=[\"Survived\"]\n",
    "#data1 variables de la data original\n",
    "data1=['Sex','Pclass', 'Embarked', 'Titulo','SibSp', 'Parch', 'Age', 'Fare', 'TamFamilia', 'IsAlone']\n",
    "data2=[\"SexCode\",\"Pclass\",\"EmbarkedCode\",\"TituloCode\",'SibSp','Parch', 'Age', 'Fare']\n",
    "data3=objetivo+data2\n",
    "\n",
    "data4=[\"SexCode\",\"Pclass\",\"EmbarkedCode\",\"TituloCode\",\"TamFamilia\",\"AgeBinCode\",\"FareBinCoce\"]\n",
    "data5=objetivo+data4\n",
    "data1_x_bin=[\"SexCode\",\"Pclass\",\"EmbarkedCode\",\"TituloCode\",\"TamFamilia\",\"AgeBinCode\",\"FareBinCode\"]\n",
    "data1_xy_bin=objetivo+data1_x_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_dummy=pd.get_dummies(data_copy[data1])\n",
    "data1_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada1_x,test1_x,entrada1_y,test1_y=model_selection.train_test_split(data_copy[data2],data_copy[objetivo],random_state=0)\n",
    "entrada1_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data1:\n",
    "    if data_copy[i].dtype!=\"float64\":\n",
    "        print(f\"Correlecacion de sobrevivientes para \",i)\n",
    "        print(data_copy[[i,objetivo[0]]].groupby(i,as_index=False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,12])\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.boxplot(x=data_copy['Fare'], showmeans = True, meanline = True)\n",
    "plt.title('Fare Boxplot')\n",
    "plt.ylabel('Fare ($)')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.boxplot(data_copy['Age'], showmeans = True, meanline = True)\n",
    "plt.title('Edad Boxplot')\n",
    "plt.ylabel('Age (Years)')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.boxplot(data_copy['TamFamilia'], showmeans = True, meanline = True)\n",
    "plt.title('Family Size Boxplot')\n",
    "plt.ylabel('Family Size (#)')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(x = [data_copy[data_copy['Survived']==1]['Fare'], data_copy[data_copy['Survived']==0]['Fare']], \n",
    "         stacked=True, color = ['g','r'],label = ['Sobrevivio','Murio'])\n",
    "plt.title('Histograma Fare by Survival')\n",
    "plt.xlabel('Fare ($)')\n",
    "plt.ylabel('# de pasajeros')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(x = [data_copy[data_copy['Survived']==1]['Age'], data_copy[data_copy['Survived']==0]['Age']], \n",
    "         stacked=True, color = ['g','r'],label = ['Sobrevivio','Murio'])\n",
    "plt.title('Histograma Age by Survival')\n",
    "plt.xlabel('Age (Años)')\n",
    "plt.ylabel('# de pasajeros')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(x = [data_copy[data_copy['Survived']==1]['TamFamilia'], data_copy[data_copy['Survived']==0]['TamFamilia']], \n",
    "         stacked=True, color = ['g','r'],label = ['Sobrevivio','Murio'])\n",
    "plt.title('Histograma TamFamilia vs Survival')\n",
    "plt.xlabel('TamFamilia (#)')\n",
    "plt.ylabel('# de pasajeros')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, saxis = plt.subplots(2, 3,figsize=(20,12))\n",
    "\n",
    "sns.barplot(x = 'Embarked', y = 'Survived', data=data_copy, ax = saxis[0,0])\n",
    "sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data_copy, ax = saxis[0,1])\n",
    "sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data_copy, ax = saxis[0,2])\n",
    "\n",
    "sns.pointplot(x = 'FareBin', y = 'Survived',  data=data_copy, ax = saxis[1,0])\n",
    "sns.pointplot(x = 'AgeBin', y = 'Survived',  data=data_copy, ax = saxis[1,1])\n",
    "sns.pointplot(x = 'TamFamilia', y = 'Survived', data=data_copy, ax = saxis[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(18,12))\n",
    "\n",
    "sns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data_copy, ax = axis1)\n",
    "axis1.set_title('Comparación de Sobrevimientes: Pclass vs Fare')\n",
    "\n",
    "sns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data_copy, split = True, ax = axis2)\n",
    "axis2.set_title('Comparación de Sobrevimientes: Pclass vs Age')\n",
    "\n",
    "sns.boxplot(x = 'Pclass', y ='TamFamilia', hue = 'Survived', data = data_copy, ax = axis3)\n",
    "axis3.set_title('Comparación de Sobrevimientes: Pclass vs TamFamilia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, qaxis = plt.subplots(1,3,figsize=(20,12))\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data_copy, ax = qaxis[0])\n",
    "axis1.set_title('Comparación de Sobrevimientes: Sex vs Embarked')\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data_copy, ax  = qaxis[1])\n",
    "axis1.set_title('Comparación de Sobrevimientes: Sex vs Pclass')\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data_copy, ax  = qaxis[2])\n",
    "axis1.set_title('Comparación de Sobrevimientes: Sex vs IsAlone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = sns.FacetGrid(data_copy, col = 'Embarked',height=5)\n",
    "e.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\n",
    "e.add_legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sns.FacetGrid( data_copy, hue = 'Survived', aspect=5 )\n",
    "a.map(sns.kdeplot, 'Age', shade= True )\n",
    "a.set(xlim=(0 , data_copy['Age'].max()))\n",
    "a.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = sns.FacetGrid(data_copy, row = 'Sex', col = 'Pclass', hue = 'Survived',height=5)\n",
    "h.map(plt.hist, 'Age', alpha = .75)\n",
    "h.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = sns.pairplot(data_copy, hue = 'Survived', palette = 'deep', height=2.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
    "pp.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "        \n",
    "    )\n",
    "    \n",
    "    plt.title('Matriz de correlación', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA = [\n",
    "    #Métodos de ensamble\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Proceso gausiano\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #Modelos de Regresión\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Modelo Bayesiano\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Método de los vecinos más cercanos\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #Máquinas de Vector Soporte\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Arboles de decisión\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Análisis discriminante lineal\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    #Extreme Gradient Boosting\n",
    "    XGBClassifier()    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizamos model_selection.ShuffleSplit para producir índices para dividir los datos en conjuntos de entrenamiento y prueba.\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n",
    "cv_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos los headers con los que trabajaremos\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA_predict = data_copy[objetivo]\n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #definimos nombres y parámetros\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #modelo de puntuación con validación cruzada:\n",
    "    cv_results = model_selection.cross_validate(alg, data_copy[data1_x_bin], data_copy[objetivo], cv  = cv_split)\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #si se trata de una muestra aleatoria sin sesgo, entonces +/-3 desviaciones estándar (estándar) de la media\n",
    "    #deberían capturar estadísticamente el 99,7 % de los subconjuntos\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n",
    "\n",
    "    #guardamos las predicciones de MLA\n",
    "    alg.fit(data_copy[data1_x_bin], data_copy[objetivo])\n",
    "    MLA_predict[MLA_name] = alg.predict(data_copy[data1_x_bin])\n",
    "    \n",
    "    row_index+=1\n",
    "\n",
    "    \n",
    "#imprimimos y ordenamos la tabla\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eficiencia de cada modelo\n",
    "\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "plt.title('Puntaje de precisión del algoritmo de aprendizaje automático \\n')\n",
    "plt.xlabel('Puntuacion de precision (%)')\n",
    "plt.ylabel('Algoritmo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  ix in data_copy.index:\n",
    "    row=data_copy.loc[ix]\n",
    "    \n",
    "    #generamos números aleatorios en Random_Predict\n",
    "    if random.random() > .5:     # numeros entre 0.0 <= x < 1.0   \n",
    "        data_copy.loc[ix,'Random_Predict'] =1\n",
    "    else: \n",
    "        data_copy.loc[ix,'Random_Predict'] =0    \n",
    "\n",
    "#puntuación conjetura aleatoria de supervivencia. Utilice el método abreviado 1 = Suposición correcta y 0 = Suposición incorrecta. \n",
    "#La media de la columna será igual a la precisión.\n",
    "data_copy['Random_Score'] = 0 #Asume la predicción\n",
    "data_copy.loc[(data_copy['Survived'] == data_copy['Random_Predict']), 'Random_Score'] = 1 #Correcta predicción\n",
    "print('Precisión del modelo Coin Flip: {:.2f}%'.format(data_copy['Random_Score'].mean()*100))\n",
    "\n",
    "#también podemos usar la función precision_score de scikit para ahorrarnos algunas líneas de código\n",
    "print('Precisión del modelo Coin Flip con SciKit: {:.2f}%'.format(metrics.accuracy_score(data_copy['Survived'], data_copy['Random_Predict'])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_female = data_copy[data_copy.Sex=='female'].groupby(['Sex','Titulo'])['Survived'].mean()\n",
    "print('Árbol de decisión de supervivencia con nodo femenino: \\n',pivot_female)\n",
    "\n",
    "pivot_male = data_copy[data_copy.Sex=='male'].groupby(['Sex','Titulo'])['Survived'].mean()\n",
    "print('\\n\\nÁrbol de decisión de supervivencia con nodo masculino: \\n',pivot_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.iartificial.net/precision-recall-f1-accuracy-en-clasificacion/#Recall_Exhaustividad\n",
    "\n",
    "def mytree(df):\n",
    "    \n",
    "   # inicializar tabla para almacenar predicciones\n",
    "    Model = pd.DataFrame(data = {'Predict':[]})\n",
    "    male_title = ['Master'] # títulos de sobrevivientes\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "         # Pregunta 1: ¿Estuviste en el Titanic; la mayoría murió\n",
    "        Model.loc[index, 'Predict'] = 0\n",
    "\n",
    "        # Pregunta 2: ¿Eres mujer? la mayoría sobrevivió\n",
    "        if (df.loc[index, 'Sex'] == 'female'):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "\n",
    "              \n",
    "        if ((df.loc[index, 'Sex'] == 'female') & \n",
    "            (df.loc[index, 'Pclass'] == 3) & \n",
    "            (df.loc[index, 'Embarked'] == 'S')  &\n",
    "            (df.loc[index, 'Fare'] > 8)\n",
    "\n",
    "           ):\n",
    "                  Model.loc[index, 'Predict'] = 0 #Muere\n",
    "\n",
    "        if ((df.loc[index, 'Sex'] == 'male') &\n",
    "            (df.loc[index, 'Titulo'] in male_title)\n",
    "            ):\n",
    "            Model.loc[index, 'Predict'] = 1 #Vive\n",
    "                \n",
    "    return Model\n",
    "\n",
    "Tree_Predict = mytree(data_copy)\n",
    "print('Exactitud/puntuación de precisión del modelo del árbol de decisiones: {:.2f}%\\n'.format(metrics.accuracy_score(data_copy['Survived'], Tree_Predict)*100))\n",
    "\n",
    "print(metrics.classification_report(data_copy['Survived'], Tree_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de confución\n",
    "\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Matriz de confusion',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Esta función imprime y traza la matriz de confusión.\n",
    "    La normalización se puede aplicar estableciendo 'normalize=True.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Matriz de confusión normalizada\")\n",
    "    else:\n",
    "        print('Matriz de confusión, sin normalización')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Valores reales')\n",
    "    plt.xlabel('Valores de predicción')\n",
    "\n",
    "# Cálculo de la matriz de confusión\n",
    "cnf_matrix = metrics.confusion_matrix(data_copy['Survived'], Tree_Predict)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = ['Muertes', 'Sobrevivientes']\n",
    "# Matriz de confusión\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Matriz de confusión, sin normalización')\n",
    "\n",
    "# Matriz de confusión normalizada\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n",
    "                      title='Matriz de confusión normalizada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, data_copy[data1_x_bin], data_copy[objetivo], cv  = cv_split)\n",
    "#Creemos un clasificador de árbol de decisiones a partir del conjunto de entrenamiento (x, y).\n",
    "dtree.fit(data_copy[data1_x_bin], data_copy[objetivo])\n",
    "\n",
    "print('Antes de los parámetros DT: ', dtree.get_params())\n",
    "print(\"Antes de la prueba DT con media de puntuación bin: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"Antes de la prueba DT con puntuación de bin 3 * std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "\n",
    "print('-'*10)\n",
    "\n",
    "#Entrenamos la data\n",
    "param_grid = {'criterion': ['gini', 'entropy'], \n",
    "\n",
    "              'max_depth': [2,4,6,8,10,None], \n",
    "\n",
    "              'random_state': [0] \n",
    "             }\n",
    "\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "tune_model.fit(data_copy[data1_x_bin], data_copy[objetivo])\n",
    "\n",
    "\n",
    "print('Después de los parámetros DT: ', tune_model.best_params_)\n",
    "\n",
    "print(\"Después de la prueba DT con media de puntuación bin: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"Después de la prueba DT con puntuación bin 3 * std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprimimos columnas sin entrenar\n",
    "print('Antes de la prueba DT RFE: ', data_copy[data1_x_bin].shape) \n",
    "print('Antes de la prueba DT RFE Formación Columnas Antiguo: ', data_copy[data1_x_bin].columns.values)\n",
    "\n",
    "\n",
    "print(\"Antes de la prueba DT RFE con la media de la puntuación del intervalo: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"Antes de la prueba DT RFE con puntuación bin 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#selección de columnas y entrenamiento\n",
    "dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\n",
    "dtree_rfe.fit(data_copy[data1_x_bin], data_copy[objetivo])\n",
    "\n",
    "#transformamos x&y en características reducidas para adaptarse al nuevo modelo\n",
    "X_rfe = data_copy[data1_x_bin].columns.values[dtree_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(dtree, data_copy[X_rfe], data_copy[objetivo], cv  = cv_split)\n",
    "\n",
    "print('Después de la forma de entrenamiento DT RFE Nuevo: ', data_copy[X_rfe].shape) \n",
    "print('Después de las columnas de entrenamiento DT RFE Nuevo: ', X_rfe)\n",
    "\n",
    "print(\"Después de la prueba DT RFE con promedio de puntaje de bin: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"Después de la prueba DT RFE con puntuación bin 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#Entrenamos el nuevo modelo con el metodo fit\n",
    "rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(criterion='entropy',\n",
    "                                            min_samples_split=20,\n",
    "                                            min_samples_leaf=5,\n",
    "                                            max_depth = 4,\n",
    "                                            class_weight={1:3.5}), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "rfe_tune_model.fit(data_copy[X_rfe], data_copy[objetivo])\n",
    "\n",
    "print('Después de los parámetros elegidos DT RFE: ', rfe_tune_model.best_params_)\n",
    "\n",
    "print(\"Después de la prueba DT RFE con promedio de puntaje de bin: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"Después de la prueba DT RFE con puntuación bin 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de correlación\n",
    "\n",
    "correlation_heatmap(MLA_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_est = [\n",
    "    #Meteodos de Ensable\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc',ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "\n",
    "    #Proceso gausiano\n",
    "    ('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    \n",
    "    #Modelos de Regresión\n",
    "    ('lr', linear_model.LogisticRegressionCV()),\n",
    "    \n",
    "    #Modelo Bayesiano\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    \n",
    "    #Método de los vecinos más cercanos\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    \n",
    "    #Máquinas de Vector Soporte\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    \n",
    "    #Extreme Gradient Boosting\n",
    "   ('xgb', XGBClassifier())\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "#Reglas de mayoría\n",
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, data_copy[data1_x_bin], data_copy[objetivo], cv  = cv_split)\n",
    "vote_hard.fit(data_copy[data1_x_bin], data_copy[objetivo])\n",
    "\n",
    "\n",
    "print(\"Prueba de votación de reglas de mayoría w/bin con promedio: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Prueba de votación de reglas de mayoría w/bin con 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#Probabilidades ponderadas\n",
    "vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "vote_soft_cv = model_selection.cross_validate(vote_soft, data_copy[data1_x_bin], data_copy[objetivo], cv  = cv_split)\n",
    "vote_soft.fit(data_copy[data1_x_bin], data_copy[objetivo])\n",
    "\n",
    "\n",
    "print(\"Prueba de votación de probabilidades ponderadas w/bin con promedio:\". format(vote_soft_cv['test_score'].mean()*100))\n",
    "print(\"Prueba de votación de probabilidades ponderadas w/bin con 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organización de hiperparámetros con GridSearchCV\n",
    "\n",
    "grid_n_estimator = [10, 50, 100, 300]\n",
    "grid_ratio = [.1, .25, .5, .75, 1.0]\n",
    "grid_learn = [.01, .03, .05, .1, .25]\n",
    "grid_max_depth = [2, 4, 6, 8, 10, None]\n",
    "grid_min_samples = [5, 10, .03, .05, .10]\n",
    "grid_criterion = ['gini', 'entropy']\n",
    "grid_bool = [True, False]\n",
    "grid_seed = [0]\n",
    "\n",
    "\n",
    "grid_param = [\n",
    "            [{\n",
    "            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "            'n_estimators': grid_n_estimator,\n",
    "            'learning_rate': grid_learn,\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "       \n",
    "    \n",
    "            [{\n",
    "            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "            'n_estimators': grid_n_estimator,\n",
    "            'max_samples': grid_ratio,\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "            'n_estimators': grid_n_estimator,\n",
    "            'criterion': grid_criterion,\n",
    "            'max_depth': grid_max_depth,\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "\n",
    "            [{\n",
    "            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "            'learning_rate': [.05],\n",
    "            'n_estimators': [300],\n",
    "            'max_depth': grid_max_depth,\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "            'n_estimators': grid_n_estimator,\n",
    "            'criterion': grid_criterion,\n",
    "            'max_depth': grid_max_depth,\n",
    "            'oob_score': [True],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "    \n",
    "            [{\n",
    "            #GaussianProcessClassifier\n",
    "            'max_iter_predict': grid_n_estimator, #por defecto: 100\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "        \n",
    "    \n",
    "            [{\n",
    "            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "            'fit_intercept': grid_bool,\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "            \n",
    "    \n",
    "            [{\n",
    "            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "            'alpha': grid_ratio,\n",
    "             }],\n",
    "\n",
    "    \n",
    "            #GaussianNB - \n",
    "            [{}],\n",
    "    \n",
    "    \n",
    "            [{\n",
    "            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "            'n_neighbors': [1,2,3,4,5,6,7],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            }],\n",
    "            \n",
    "    \n",
    "            [{\n",
    "            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "            'C': [1,2,3,4,5],\n",
    "            'gamma': grid_ratio,\n",
    "            'decision_function_shape': ['ovo', 'ovr'],\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "             }],\n",
    "\n",
    "    \n",
    "            [{\n",
    "            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            'learning_rate': grid_learn, \n",
    "            'max_depth': [1,2,4,6,8,10],\n",
    "            'n_estimators': grid_n_estimator, \n",
    "            'seed': grid_seed  \n",
    "             }]   \n",
    "        ]\n",
    "\n",
    "\n",
    "start_total = time.perf_counter()\n",
    "for clf, param in zip (vote_est, grid_param):\n",
    "\n",
    "    start = time.perf_counter()        \n",
    "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n",
    "    best_search.fit(data_copy[data1_x_bin], data_copy[objetivo])\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    best_param = best_search.best_params_\n",
    "    print('El mejor parámetro para {} es {} con un tiempo de ejeción de {:.2f} segundos.'.format(clf[1].__class__.__name__, best_param, run))\n",
    "    clf[1].set_params(**best_param) \n",
    "\n",
    "\n",
    "run_total = time.perf_counter() - start_total\n",
    "print('El tiempo total de optimización fue {:.2f} minutos.'.format(run_total/60))\n",
    "\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elección fija o reglas de mayoría con con hiperparámetros ajustados\n",
    "grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "grid_hard_cv = model_selection.cross_validate(grid_hard, data_copy[data1_x_bin], data_copy[objetivo], cv  = cv_split)\n",
    "grid_hard.fit(data_copy[data1_x_bin], data_copy[objetivo])\n",
    "\n",
    "\n",
    "print(\"Elección fija con prueba de hiperparámetros organizados con la media: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n",
    "print(\"Elección fija con prueba de hiperparámetros organizados con 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "#Elección flexible o probabilidades ponderadas con hiperparámetros ajustados\n",
    "grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "grid_soft_cv = model_selection.cross_validate(grid_soft, data_copy[data1_x_bin], data_copy[objetivo], cv  = cv_split)\n",
    "grid_soft.fit(data_copy[data1_x_bin], data_copy[objetivo])\n",
    "\n",
    "\n",
    "print(\"Elección flexible con prueba de hiperparámetros organizados con la media: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n",
    "print(\"Elección flexible con prueba de hiperparámetros organizados con 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparamos la data para el modelo\n",
    "print(data_test.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#Mostramos los resultados y los convertimos a enteros\n",
    "data_test['Survived'] = mytree(data_test).astype(int)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = data_test[['PassengerId','Survived']]\n",
    "submit.to_csv(\"resultado_prediccion.csv\", index=False)\n",
    "\n",
    "print('Conteo de datos validados: \\n', data_test['Survived'].value_counts(normalize = True))\n",
    "submit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
